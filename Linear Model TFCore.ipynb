{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeM9fA+/pP3UE+P/VhRbqG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9characters/GAN_repo/blob/master/Linear%20Model%20TFCore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjxZyXHi-4lp",
        "colab_type": "text"
      },
      "source": [
        "## **Training Linear Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hIsKXAiU_33S",
        "outputId": "ae5dea0a-4868-402f-8434-99c69c20a007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "#Trainable Parameters\n",
        "W = tf.Variable([1], dtype = tf.float32)\n",
        "b = tf.Variable([-1], dtype = tf.float32)\n",
        "\n",
        "#Training data (Inputs/Outputs)\n",
        "x = tf.placeholder(dtype = tf.float32)\n",
        "y = tf.placeholder(dtype = tf.float32)\n",
        "\n",
        "#Linear Model\n",
        "linear_model = W * x + b\n",
        "\n",
        "#Linear Regression loss function - Sum of Squares\n",
        "squared_deltas = tf.square(linear_model - y)\n",
        "loss = tf.reduce_sum(squared_deltas)\n",
        "\n",
        "#Creating a session\n",
        "sess = tf.Session()\n",
        "\n",
        "#Initializing Variables\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "#Print the loss\n",
        "print(sess.run(loss, feed_dict = {x: [1,2,3,4], y: [0,1,2,3]}))\n",
        "\n",
        "#Closing the session\n",
        "sess.close()\n",
        "\n",
        "###reduce_sum(input_tensor, axis=None, keep_dims=False, name=None,reduction_indices=None)\n",
        "###Returns the sum of all elements across the specified axis. If no axis in specified, all dimensions are reduced to return a Tensor with just one element.\n",
        "   \n",
        "###square(x, name=None)\n",
        "###Calculates the square of x elementwise"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QXTAK19D4-K",
        "colab_type": "text"
      },
      "source": [
        "Note: The tensor used for evaluation above is the Tensor \"loss\" not linear_model because it is the last Tensor at the top of the graph.\n",
        "The loss returned is 96. The existance of the error, especially for the large error, means that the parameters must be updated.\n",
        "Theses parameters are expected to be updated automatically, but we can start updating it manually until reaching zero error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxWfUbEJIRu8",
        "colab_type": "text"
      },
      "source": [
        "When W = 1.0 and b=-1.0, the desired result is identical to the predicted resuls and thus we can't enhace the model more than this. We reached the optimal values for the parameters but in manual way. We need to make it done automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmmqEmJOP3P9",
        "colab_type": "text"
      },
      "source": [
        "There are a number of optimizers already exist in TensorFlow for making things simpler. These optimizers exist in APIs in TensorFlow like:\n",
        "\n",
        "\n",
        "*   tensorflow.train\n",
        "*   tensorflow.estimator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB55RPToQUuc",
        "colab_type": "text"
      },
      "source": [
        "Now we use the TensorFlow optimizers to calculate the best values of the model parameters automatically.\n",
        "\n",
        "The simplest optimizer is the gradient descent that changes the values of each parameter slowly until reaching the value that minimizes the loss. Gradient descent modifies each value according to the magnitude of the derivative of loss with respect to the variable. Because due to such operations of calculating the gradients is complex and error-prone, Tensorflow can calculate the gradients automatically. \n",
        "\n",
        "After calculating the gradients, you need to optimize the parameters yourself. But TensorFlow makes things more easier by providing optimizers that will calculate the derivatives in addition to optimizing the parameters.\n",
        "\n",
        "tensorflow.train API contains a class called GradientDescentOptimizer that can both calculate the derivatives and optimizing the parameters. The GradientDescentOptimizer has the following constructor:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "__init__(\n",
        "  learning_rate,\n",
        "  use_locking=False,\n",
        "  name='GradientDescent'\n",
        ")\n",
        "```\n",
        "For example, the following code shows how to minimize the loss using the GradientDescentOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwROcJFiphEb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "860be51c-e63e-41e6-ad1d-4ac914318e21"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "#Trainable parameters\n",
        "W = tf.Variable([0.3], dtype=tf.float32)\n",
        "b = tf.Variable([-0.2], dtype=tf.float32)\n",
        "\n",
        "#Training Data (Inputs/Outputs)\n",
        "x = tf.placeholder(dtype=tf.float32)\n",
        "y = tf.placeholder(dtype=tf.float32)\n",
        "\n",
        "x_train = [1,2,3,4]\n",
        "y_train = [0,1,2,3]\n",
        "\n",
        "#Linear Model\n",
        "linear_model = W * x + b\n",
        "\n",
        "#Linear Regression Loss - Sum of Squares\n",
        "squared_deltas = tf.square(linear_model - y)\n",
        "loss = tf.reduce_sum(squared_deltas)\n",
        "\n",
        "#Gradient Descent Optimizer\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "train = optimizer.minimize(loss=loss)\n",
        "\n",
        "#Creating a Session\n",
        "sess = tf.Session()\n",
        "writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
        "\n",
        "#Initializing the Variables\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "#Optimizing the parameters\n",
        "for i in range(50):\n",
        "  print(f\"Iteration {i}:\\n\")\n",
        "  sess.run(train, {x: x_train, y: y_train})\n",
        "\n",
        "  #Print the parameters and loss\n",
        "  curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n",
        "  print(f\"W: {curr_W}, b: {curr_b}, loss: {curr_loss}\")\n",
        "\n",
        "writer.close()\n",
        "sess.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0:\n",
            "\n",
            "W: [0.56], b: [-0.12400001], loss: 1.1687040328979492\n",
            "Iteration 1:\n",
            "\n",
            "W: [0.6488], b: [-0.10608], loss: 0.6177209615707397\n",
            "Iteration 2:\n",
            "\n",
            "W: [0.680736], b: [-0.10735361], loss: 0.5453583002090454\n",
            "Iteration 3:\n",
            "\n",
            "W: [0.6937651], b: [-0.11491252], loss: 0.5260202884674072\n",
            "Iteration 4:\n",
            "\n",
            "W: [0.70048857], b: [-0.12447254], loss: 0.5127966403961182\n",
            "Iteration 5:\n",
            "\n",
            "W: [0.7050899], b: [-0.13461244], loss: 0.5005109310150146\n",
            "Iteration 6:\n",
            "\n",
            "W: [0.70895845], b: [-0.14486143], loss: 0.4885863661766052\n",
            "Iteration 7:\n",
            "\n",
            "W: [0.71255565], b: [-0.15506421], loss: 0.4769531190395355\n",
            "Iteration 8:\n",
            "\n",
            "W: [0.71603507], b: [-0.16517021], loss: 0.465597927570343\n",
            "Iteration 9:\n",
            "\n",
            "W: [0.7194481], b: [-0.17516361], loss: 0.45451292395591736\n",
            "Iteration 10:\n",
            "\n",
            "W: [0.72281194], b: [-0.18504015], loss: 0.443692147731781\n",
            "Iteration 11:\n",
            "\n",
            "W: [0.7261328], b: [-0.19479932], loss: 0.43312883377075195\n",
            "Iteration 12:\n",
            "\n",
            "W: [0.729413], b: [-0.20444193], loss: 0.4228169322013855\n",
            "Iteration 13:\n",
            "\n",
            "W: [0.73265356], b: [-0.21396917], loss: 0.41275060176849365\n",
            "Iteration 14:\n",
            "\n",
            "W: [0.7358553], b: [-0.22338235], loss: 0.4029237926006317\n",
            "Iteration 15:\n",
            "\n",
            "W: [0.73901856], b: [-0.23268282], loss: 0.39333105087280273\n",
            "Iteration 16:\n",
            "\n",
            "W: [0.742144], b: [-0.24187191], loss: 0.38396671414375305\n",
            "Iteration 17:\n",
            "\n",
            "W: [0.745232], b: [-0.25095096], loss: 0.37482544779777527\n",
            "Iteration 18:\n",
            "\n",
            "W: [0.74828297], b: [-0.25992128], loss: 0.3659015893936157\n",
            "Iteration 19:\n",
            "\n",
            "W: [0.7512975], b: [-0.26878417], loss: 0.35719022154808044\n",
            "Iteration 20:\n",
            "\n",
            "W: [0.7542758], b: [-0.27754092], loss: 0.3486863970756531\n",
            "Iteration 21:\n",
            "\n",
            "W: [0.7572185], b: [-0.2861928], loss: 0.34038496017456055\n",
            "Iteration 22:\n",
            "\n",
            "W: [0.76012594], b: [-0.29474106], loss: 0.3322811722755432\n",
            "Iteration 23:\n",
            "\n",
            "W: [0.7629986], b: [-0.30318695], loss: 0.32437023520469666\n",
            "Iteration 24:\n",
            "\n",
            "W: [0.76583683], b: [-0.31153172], loss: 0.3166477084159851\n",
            "Iteration 25:\n",
            "\n",
            "W: [0.76864105], b: [-0.31977656], loss: 0.30910900235176086\n",
            "Iteration 26:\n",
            "\n",
            "W: [0.7714117], b: [-0.32792264], loss: 0.30174973607063293\n",
            "Iteration 27:\n",
            "\n",
            "W: [0.7741492], b: [-0.33597118], loss: 0.2945657968521118\n",
            "Iteration 28:\n",
            "\n",
            "W: [0.7768539], b: [-0.34392333], loss: 0.2875528335571289\n",
            "Iteration 29:\n",
            "\n",
            "W: [0.77952623], b: [-0.35178024], loss: 0.28070685267448425\n",
            "Iteration 30:\n",
            "\n",
            "W: [0.78216654], b: [-0.35954306], loss: 0.2740238308906555\n",
            "Iteration 31:\n",
            "\n",
            "W: [0.78477526], b: [-0.36721292], loss: 0.2674999535083771\n",
            "Iteration 32:\n",
            "\n",
            "W: [0.7873527], b: [-0.37479094], loss: 0.2611313462257385\n",
            "Iteration 33:\n",
            "\n",
            "W: [0.78989923], b: [-0.3822782], loss: 0.25491437315940857\n",
            "Iteration 34:\n",
            "\n",
            "W: [0.7924153], b: [-0.3896758], loss: 0.24884548783302307\n",
            "Iteration 35:\n",
            "\n",
            "W: [0.7949013], b: [-0.3969848], loss: 0.24292096495628357\n",
            "Iteration 36:\n",
            "\n",
            "W: [0.7973575], b: [-0.40420628], loss: 0.2371375411748886\n",
            "Iteration 37:\n",
            "\n",
            "W: [0.79978424], b: [-0.41134128], loss: 0.23149177432060242\n",
            "Iteration 38:\n",
            "\n",
            "W: [0.80218196], b: [-0.41839084], loss: 0.2259804606437683\n",
            "Iteration 39:\n",
            "\n",
            "W: [0.80455095], b: [-0.42535597], loss: 0.22060033679008484\n",
            "Iteration 40:\n",
            "\n",
            "W: [0.80689156], b: [-0.43223768], loss: 0.21534836292266846\n",
            "Iteration 41:\n",
            "\n",
            "W: [0.80920416], b: [-0.439037], loss: 0.21022143959999084\n",
            "Iteration 42:\n",
            "\n",
            "W: [0.81148905], b: [-0.44575486], loss: 0.20521646738052368\n",
            "Iteration 43:\n",
            "\n",
            "W: [0.8137466], b: [-0.45239228], loss: 0.2003307342529297\n",
            "Iteration 44:\n",
            "\n",
            "W: [0.8159771], b: [-0.45895022], loss: 0.19556128978729248\n",
            "Iteration 45:\n",
            "\n",
            "W: [0.8181809], b: [-0.46542963], loss: 0.1909053474664688\n",
            "Iteration 46:\n",
            "\n",
            "W: [0.8203583], b: [-0.47183144], loss: 0.18636032938957214\n",
            "Iteration 47:\n",
            "\n",
            "W: [0.8225096], b: [-0.4781566], loss: 0.1819235384464264\n",
            "Iteration 48:\n",
            "\n",
            "W: [0.82463515], b: [-0.484406], loss: 0.17759230732917786\n",
            "Iteration 49:\n",
            "\n",
            "W: [0.82673526], b: [-0.49058056], loss: 0.17336422204971313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61blrA8F13LL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard --logdir='./graphs'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvtJd54vtAE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS5zp6c-1oLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir=graphs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaGwYEhc8Y63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}